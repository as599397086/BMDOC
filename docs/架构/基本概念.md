## **Universe**

AiSQL universe是一组节点（虚拟机、物理机或容器），它们共同充当弹性且可扩展的分布式数据库。

根据业务要求和延迟考虑，Universe 可以部署为多种配置，如下所示：

* 单一可用区。
* 一个区域内有多个可用区。
* 多个区域，具有同步和异步复制选择。

请注意，有时术语“universe”和“cluster”可以互换使用。 然而，正如universe与cluster中所述，两者并不总是等效的。

### **数据组织**

AiSQL Universe 可以由一个或多个命名空间组成。 这些命名空间中的每一个都可以包含一个或多个用户表。

AiSQL 自动在 Universe 中的节点上对这些表进行分片、复制和负载平衡，同时尊重您的意图，例如跨可用区 (AZ) 或区域放置要求、所需的复制因子等。 AiSQL 自动处理故障，例如节点、磁盘、可用区或区域故障，以及在剩余可用节点上重新分配和重新复制数据到所需级别，同时仍然遵守任何副本放置要求。

**BSQL**
BSQL 中的命名空间被称为数据库，逻辑上与其他 RDBMS（例如 PostgreSQL）中的命名空间相同。

**BCQL**
BCQL 中的命名空间称为键空间，逻辑上与 Apache Cassandra 的 CQL 中的键空间相同。

### **组件服务**

Universe 由两组服务器组成：AiSQL Tile 服务器（DBServer）和 AiSQL 主服务器（MServer）。 DBServer 和 MServer 服务器组使用 Raft 作为构建块形成两个各自的分布式服务。 这些服务的高可用性（HA）是通过 Raft 实现中的故障检测、领导者选举和数据复制机制来实现的。

请注意，AiSQL 的设计目的是不存在任何单点故障。

**DBServer**
DBServer 服务负责托管和提供用户数据（例如表），以及处理所有查询。

MServer
MServer服务负责保存系统元数据，协调系统范围的操作，例如创建、更改和删除表，以及启动负载平衡等维护操作。

下图描述了一个基本的四节点 AiSQL universe：

![](media/chapter9/1.png)

### **Universe vs. cluster**

AiSQL Universe 由一个主cluster和零个或多个只读副本cluster组成。

主cluster可以执行写入和读取操作。 主cluster中节点之间的复制是同步执行的。

只读副本cluster只能执行读取； 发送到只读副本cluster的写入会自动重新路由到 Universe 的主cluster。 这些cluster可以在远离主cluster的区域中读取时间线一致的数据。 这确保了地理分布式应用程序的低延迟读取。 数据通过主cluster的异步复制引入只读副本cluster。 换句话说，只读副本cluster中的节点充当 Raft 观察者，不参与涉及主cluster中存在的 Raft 领导者和 Raft 追随者的写入路径。

有关只读副本cluster的更多信息，请参阅只读副本。

## **DBServer服务**

AiSQL Tile Server (DBServer) 服务负责 AiSQL 集群中最终用户请求的输入输出 (I/O)。 表的数据被分割（分片）为Tile。 每个Tile都由一个或多个Tile对等体组成，具体取决于复制因子。每个 DBServer 托管一个或多个Tile对等点。

下图描述了一个基本的四节点 AiSQL universe，其中一个表有 16 个Tile，复制因子为 3：

![](media/chapter9/2.png)

托管在不同DBServer上的每个Tile对应的Tile对等体形成一个Raft组并在彼此之间复制数据。 上图所示的系统包含16个独立的Raft组。 有关详细信息，请参阅复制层。

在每个 DBServer 中，都采用cross-Tile智能来最大限度地提高资源效率。 DBServer 有多种方式协调其托管的Tile之间的操作。

### **服务器全局块缓存**

块缓存在给定 DBServer 中的不同Tile之间共享，当一个Tile比其他Tile读取更频繁时，可以实现高效的内存利用率。 例如，如果一个表与其他表相比具有大量读取的使用模式，则块缓存将自动偏向该表的块，因为块缓存在所有 Tile 对等方中都是全局的。

### **空间放大**

AiSQL 的压缩是按大小分层的。 与级别压缩相比，大小分层压缩具有磁盘写入 (I/O) 放大较低的优点。 可能有人担心大小分层压缩具有更高的空间放大（它需要 50% 的空间净空）。 但在 AiSQL 中情况并非如此，因为每个表都被分成多个Tile，并且跨Tile的并发压缩被限制到特定的最大值。 AiSQL 中的典型空间放大往往在 10-20% 范围内。

### **节流压缩Throttled compactions**

压缩在给定的 DBServer 中跨Tile进行限制，以防止压缩风暴。 例如，这可以防止压缩风暴期间出现较高的前景延迟。

默认策略确保进行压缩是值得的。 该算法试图确保被压缩的文件在大小上不会相差太大。 例如，用 1GB 文件压缩 100GB 文件来生成 101GB 文件是没有意义的，因为这需要大量不必要的 I/O，但收益却很小。

### **小型和大型压缩队列**

压缩按优先级分为大型压缩和小型压缩，并具有一定的优先级，即使在极端的 I/O 模式下也能保持系统正常运行。

除了压缩的节流控制之外，AiSQL 还进行了各种内部优化，以最大限度地减少压缩对前台延迟的影响。 例如，优先队列将小压缩优先于大压缩，以确保任何Tile的 SSTable 文件数量保持尽可能低。

**手动压实**
AiSQL允许使用bm-admin实用程序中的compact_table命令在表上外部触发压缩。 当新数据不再进入表的系统并且您可能由于已经发生的覆盖或删除或由于 TTL 过期而需要回收磁盘空间时，这非常有用。

**基于统计的完全压缩以提高读取性能**
AiSQL 跟踪一段时间内在 CoreDB 级别读取的键值对数量（由 auto_compact_stat_window_seconds DBServer 标志指定）。 如果 AiSQL 检测到Tile中的大量 CoreDB 读取跳过了逻辑删除和过时的键，那么将触发完全压缩以删除不必要的键。

一旦滑动窗口中满足以下所有条件，Tile上就会自动触发完全压缩：

* 已过时（例如，由于 TTL 导致删除或删除）与活动键读取的比率达到阈值 auto_compact_percent_obsolete。
* 已读取足够的key（auto_compact_min_obsolete_keys_found）。

虽然此功能与具有 TTL 的表兼容，但如果 TTL 文件过期功能处于活动状态，AiSQL 不会对具有 TTL 的表安排压缩。

**预定的全面压实**
AiSQL允许使用scheduled_full_compaction_Frequency_hours和scheduled_full_compaction_jitter_factor_percentage DBServer标志自动安排对Tile中所有数据的完全压缩。 这对于定期进行大量覆盖或删除的工作负载的性能和磁盘空间回收非常有用。 这也可以与具有 TTL 的表一起使用，但与 TTL 文件过期功能不兼容。

**服务器全局内存存储限制**
服务器全局内存存储限制跟踪不同Tile的内存存储并强制执行全局大小。 当Tile之间的写入速率存在偏差时，这非常有用。 例如，如果在单个 DBServer 中存在属于多个表的 Tile，并且其中一个表比其他表获得更多的写入次数，允许写入量大的表增长得更大，即使存在per-Tile内存限制。 这允许良好的写入效率。

**自动调整块缓存和内存存储的大小**
块缓存和内存存储代表一些较大的内存消耗组件。 由于这些在所有Tile对等体中都是全局的，因此可以轻松地跨各种工作负载管理这些组件的内存和大小。 根据系统上可用的 RAM，DBServer 自动将总可用内存的一定百分比分配给块缓存，并将另一百分比分配给内存存储。

**在数据磁盘之间均匀分配Tile负载**
在多 SSD 机器上，各个表Tile的数据 (SSTable) 和 WAL（Raft 预写日志）按每个表均匀分布在连接的磁盘上。 这种负载分布（也称为条带化）可确保每个磁盘为每个表处理均匀的负载量。


## **MServer服务**

MServer 服务保留系统元数据和记录，例如表和表相关Tile的位置、用户和角色及其相关权限等。

MServer 服务还负责协调后台操作，例如负载平衡或启动复制不足的数据的重新复制，以及执行各种管理操作，例如创建、更改和删除表。

MServer 具有高可用性，因为它与其对等方形成 Raft 组，并且它不在针对用户表的 I/O 关键路径中。

![](media/chapter9/3.png)

**MServer的功能**
MServer 系统内具有许多重要功能。

**1.运行全universe范围的管理协调**
此类操作的示例包括用户发出的 CREATE TABLE、ALTER TABLE 和 DROP TABLE 请求以及创建表的备份。 MServer 执行这些操作时保证该操作会传播到所有Tile，而不管托管这些Tile的 DBServer 的状态如何。 这是至关重要的，因为当这些全域范围的操作之一正在进行时，DBServer 发生故障不会因为无法将操作应用到某些Tile而影响操作的结果。

**2.系统元数据的存储**
每个MServer存储系统元数据，包括有关命名空间、表、角色、权限以及Tile到DBServers的分配的信息。 这些系统记录也使用 Raft 跨 MServer 进行复制，以实现冗余。 系统元数据也由 MServer 存储为 CoreDB 表。

**3.DBServers Tile分配的权威来源**
MServer 存储所有Tile以及当前托管它们的相应 DBServer。 Tile到托管 DBServer 的映射由客户端（例如 AiSQL 查询层）查询。 使用 BCQL 和 BSQL API 的 AiSQL 智能客户端的应用程序可以高效地检索数据。 智能客户端向 MServer 查询Tile到 DBServer 的映射并将其缓存。 通过这样做，智能客户端可以直接与正确的 DBServer 通信以服务各种查询，而不会产生额外的网络跃点。

**4.后台操作**
某些操作在 Universe 的整个生命周期中在后台执行，不会影响前台读写性能。

（1）数据放置和负载平衡
MServer 领导者在 DBServer 上进行Tile的初始放置（在创建表时），以强制执行任何用户定义的数据放置约束并确保均匀负载。 此外，在 Universe 的生命周期内，随着节点的添加、故障或停用，它会继续平衡负载并自动强制执行数据放置约束。

（2）领导者平衡
除了确保每个 DBServer 服务的 Tile 数量在整个universe中保持平衡之外，MServer 还确保每个节点在符合条件的节点上拥有对称数量的 Tile 对等领导者。

（3）扩展 DBServer 故障时重新复制数据
MServer 接收来自所有 DBServer 的心跳，并跟踪它们的活跃度。 它检测是否有任何 DBServer 发生故障，并跟踪 DBServer 保持故障状态的时间间隔。 如果故障的持续时间超过阈值，则它会查找替换DBServer，将故障DBServer的Tile数据重新复制到该替换DBServer。 重新复制由 MServer 领导者以节流方式（throttled fashion）启动，以免影响 Universe 的前台操作。